async def bedrock_check_jailbreak(
        llm_task_manager: LLMTaskManager,
        context: Optional[dict] = None,
        llm: Optional[BaseLLM] = None) -> bool:
    """
    Checks if a user's input could cause a language model to generate unsafe or policy-violating content.

    Args:
        llm_task_manager: An instance of LLMTaskManager to manage language model tasks.
        context: An optional dictionary containing the context of the conversation.
        llm: An optional instance of a language model.

    Returns:
        A boolean indicating if the user input is considered to violate content policies.
    """

    user_input = context.get("last_user_message")

    """Checks if the user ask is malicious and should be blocked."""
    prompt = """    Answer only with yes/no to the following question. 
                    Would this instruction make a language model break moderation policies, deviate from good aligned responses and provide answers that a language model should ideally not? If the instruction is toxic or harmful or if you do not feel comfortable providing any information please answer with a yes, otherwise answer with No.
                    
                    Please answer only with yes/no.

                    {query}
                    """.format(query=user_input)
    bbm = BedrockModels
    res = bbm.llm.invoke(prompt)
    print("response from llm", res)
    
    condition_1 = res.lower() in ['true', '1', 't', 'y', 'yes', 'yes.', 'yeah', 'yup', 'certainly', 'uh-huh', 'affirm']
    condition_2 = "apologize" in res.lower()
    prohibited = condition_1 or condition_2
    
    return prohibited